# -*- coding: utf-8 -*-
"""Implicit regularization without SGD

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d8Ugi_btEVnZ6aPkvlbm0y3jencJnFmn

Here I see that hidden sparse subnetworks also show implicit rank minimization property even though the weights of the sparse subnetworks were frozen from the beginning and thus never trained using SGD. Indicates that there are some other factors at play that causes generalization.
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import os

import torch
import torch.nn as nn
import torch.nn.functional as F

"""Parameter Settings
-------------------

"""

latent_dims = 128
num_epochs = 50
batch_size = 128
capacity = 24
learning_rate = 1e-3
use_gpu = True

"""Results on MNIST
-------------------

In the paper, they have used a latent dimension of 128 and number of linear layers at the end of encoder as 8
"""

import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from torchvision.datasets import MNIST

img_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

train_dataset = MNIST(root='./data/MNIST', download=True, train=True, transform=img_transform)
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

test_dataset = MNIST(root='./data/MNIST', download=True, train=False, transform=img_transform)
test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)

"""Autoencoder Definition
-----------------------
For $\mathcal{l}$=8, there are 8 linear layers that the output of the encoder is passed through to get the latent code


"""

from __future__ import print_function
import argparse
import os
import math
import matplotlib.pyplot as plt

import numpy as np
from torch.nn.utils import prune
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
from torch.optim.lr_scheduler import CosineAnnealingLR
import torch.autograd as autograd

"""Refer to Ramanujan et. al What'ss """

class GetSubnet(autograd.Function):
    @staticmethod
    def forward(ctx, scores, k):
        # Get the supermask by sorting the scores and using the top k%
        out = scores.clone()
        _, idx = scores.flatten().sort()
        j = int((1 - k) * scores.numel())
 
        # flat_out and out access the same memory.
        flat_out = out.flatten()
        flat_out[idx[:j]] = 0
        flat_out[idx[j:]] = 1
 
        return out
 
    @staticmethod
    def backward(ctx, g):
        # send the gradient g straight-through on the backward pass.
        return g, None

## define sparsity
k = 0.5

class SupermaskLinear(nn.Linear):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
 
        # initialize the scores
        self.scores = nn.Parameter(torch.Tensor(self.weight.size()))
        nn.init.kaiming_uniform_(self.scores, a=math.sqrt(5))
 
        # NOTE: initialize the weights like this.
        nn.init.kaiming_normal_(self.weight, mode="fan_in", nonlinearity="relu")
 
        # NOTE: turn the gradient on the weights off
        self.weight.requires_grad = False
 
    def forward(self, x):
        subnet = GetSubnet.apply(self.scores.abs(), k)
        w = self.weight * subnet
        return F.linear(x, w, self.bias)
        return x

class SupermaskConvTranspose(nn.ConvTranspose2d):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        # initialize the scores
        self.scores = nn.Parameter(torch.Tensor(self.weight.size()))
        nn.init.kaiming_uniform_(self.scores, a=math.sqrt(5))

        # NOTE: initialize the weights like this.
        nn.init.kaiming_normal_(self.weight, mode="fan_in", nonlinearity="relu")

        # NOTE: turn the gradient on the weights off
        self.weight.requires_grad = False
        
    def forward(self, x):
        subnet = GetSubnet.apply(self.scores.abs(), k)
        w = self.weight * subnet
        x = F.conv_transpose2d(
            x, w, self.bias, self.stride, self.padding, self.output_padding, self.groups, self.dilation 
        )
        return x

class SupermaskConv(nn.Conv2d):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        # initialize the scores
        self.scores = nn.Parameter(torch.Tensor(self.weight.size()))
        nn.init.kaiming_uniform_(self.scores, a=math.sqrt(5))

        # NOTE: initialize the weights like this.
        nn.init.kaiming_normal_(self.weight, mode="fan_in", nonlinearity="relu")

        # NOTE: turn the gradient on the weights off
        self.weight.requires_grad = False

    def forward(self, x):
        subnet = GetSubnet.apply(self.scores.abs(), k)
        w = self.weight * subnet
        x = F.conv2d(
            x, w, self.bias, self.stride, self.padding, self.dilation, self.groups
        )
        return x

class Encoder(nn.Module):
    def __init__(self):
        super(Encoder, self).__init__()
        c = capacity
        self.conv1 = SupermaskConv(in_channels=1, out_channels=c, kernel_size=4, stride=2, padding=1) # out: c x 14 x 14
        self.conv2 = SupermaskConv(in_channels=c, out_channels=c*2, kernel_size=4, stride=2, padding=1) # out: c x 7 x 7
        self.conv3 = SupermaskConv(in_channels=c*2, out_channels=c, kernel_size=1) # out: c x 7 x 7
        self.fc = SupermaskLinear(in_features=c*7*7, out_features=latent_dims)
        
    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        x = x.view(x.size(0), -1) # flatten batch of multi-channel feature maps to a batch of feature vectors
        x = self.fc(x)
        
        return x

class LNN(nn.Module):
    def __init__(self, num_of_linear_layers, type):
        self.num_of_linear_layers = num_of_linear_layers
        super(LNN, self).__init__()

        self.linear_layer = nn.ModuleList([SupermaskLinear(in_features=latent_dims, out_features=latent_dims) for _ in range(num_of_linear_layers)])
        self.forward = self.forward_linear
        
    def forward_linear(self, x):
        for layer in self.linear_layer:
            x = layer(x)
        return x

class Decoder(nn.Module):
    def __init__(self):
        super(Decoder, self).__init__()
        c = capacity
        self.fc = SupermaskLinear(in_features=latent_dims, out_features=c*7*7)
        self.conv3 = SupermaskConvTranspose(in_channels=c, out_channels=c*2, kernel_size=1)
        self.conv2 = SupermaskConvTranspose(in_channels=c*2, out_channels=c, kernel_size=4, stride=2, padding=1)
        self.conv1 = SupermaskConvTranspose(in_channels=c, out_channels=1, kernel_size=4, stride=2, padding=1)
        
            
    def forward(self, x):
        x = self.fc(x)
        x = x.view(x.size(0), capacity, 7, 7) # unflatten batch of feature vectors to a batch of multi-channel feature maps
        x = F.relu(self.conv3(x))
        x = F.relu(self.conv2(x))
        x = torch.tanh(self.conv1(x)) # last layer before output is tanh, since the images are normalized and 0-centered
        return x
        # x = self.decoder(x)
        # x = x.view(x.size(0), 1, 28, 28)
        # return x

class Autoencoder(nn.Module):
    def __init__(self, *args, **kwargs):
        super(Autoencoder, self).__init__()
        self.encoder = Encoder()
        self.lnn = LNN(*args, **kwargs)
        self.decoder = Decoder()
    
    def forward(self, x):
        latent = self.encoder(x)
        min_rank_latent = self.lnn(latent)
        x_recon = self.decoder(min_rank_latent)
        # x_recon = self.decoder(latent)
        return x_recon

autoencoder = Autoencoder(8,'linear')

device = torch.device("cuda:0" if use_gpu and torch.cuda.is_available() else "cpu")
autoencoder = autoencoder.to(device)

num_params = sum(p.numel() for p in autoencoder.parameters() if p.requires_grad)
print('Number of parameters: %d' % num_params)

autoencoder(torch.rand(128,1,28,28).cuda()).shape

"""Train Autoencoder
--------
"""

def train(layers=0, type="linear", model=None, lr=1e-3, num_epochs=50):
    print(lr)
    if model == None:
        autoencoder = Autoencoder(layers, type)
    else:
        autoencoder = model
    
    device = torch.device("cuda:0" if use_gpu and torch.cuda.is_available() else "cpu")
    autoencoder = autoencoder.to(device)
    num_params = sum(p.numel() for p in autoencoder.parameters() if p.requires_grad)
    print('Number of parameters: %d' % num_params)
    optimizer = torch.optim.Adam(params=autoencoder.parameters(), lr=learning_rate)#, momentum=0.5)

    # set to training mode
    autoencoder.train()

    train_loss_avg = []

    print('Training ...')
    for epoch in range(num_epochs):
        train_loss_avg.append(0)
        num_batches = 0
        
        for image_batch, _ in train_dataloader:
            # print(image_batch.shape)
            image_batch = image_batch.to(device)
            
            # autoencoder reconstruction
            image_batch_recon = autoencoder(image_batch)
            
            # reconstruction error
            loss = F.mse_loss(image_batch_recon, image_batch)
            
            # backpropagation
            optimizer.zero_grad()
            loss.backward()
            
            # one step of the optmizer (using the gradients from backpropagation)
            optimizer.step()
            
            train_loss_avg[-1] += loss.item()
            num_batches += 1
            
        train_loss_avg[-1] /= num_batches
        print('Epoch [%d / %d] average reconstruction error: %f' % (epoch+1, num_epochs, train_loss_avg[-1]))
    return autoencoder, train_loss_avg

capacity = 24

standard_ae, standard_ae_train_loss_avg = train(0, 'linear', num_epochs=20)

linear_irmae = Autoencoder(8,'linear')

from copy import deepcopy

"""Creating copy of the initial parameters to verify that the weights have not changed and remained the same"""

a = deepcopy(linear_irmae.encoder.fc.weight.data.cpu().numpy())
b = deepcopy(linear_irmae.lnn.linear_layer[0].weight.data.cpu().numpy())
c = deepcopy(linear_irmae.decoder.fc.scores.data.cpu().numpy())

print(np.all(linear_irmae.encoder.fc.weight.data.cpu().numpy() == a))
print(np.all(b == linear_irmae.lnn.linear_layer[0].weight.data.cpu().numpy()))
print(np.all(c == linear_irmae.decoder.fc.scores.data.cpu().numpy()))

## They are equal initially

linear_irmae, linear_train_loss_avg = train(model=linear_irmae, lr=1e-1, num_epochs=20)

## Check again to see if the weights are still equal or not

print(np.all(a == linear_irmae.encoder.fc.weight.data.cpu().numpy()))
print(np.all(b == linear_irmae.lnn.linear_layer[0].weight.data.cpu().numpy()))
print(np.all(c == linear_irmae.decoder.fc.scores.data.cpu().numpy()))

"""Note that only the scores have changed but that does not make any difference to us because top-k% scores will determine the mask of the weights to be used. Only the weights with top-k% scores are used to find the output and since the weights haven't changed SGD should not show any effect regarding the generalization or rank minimization of the network."""

fig = plt.figure()
plt.plot(standard_ae_train_loss_avg, label="Standard")
plt.plot(linear_train_loss_avg, label="Linear")
plt.legend(loc="upper right")
plt.xlabel('Epochs')
plt.ylabel('Reconstruction error')
plt.show()
# plt.savefig("train_plot_1.png")

"""Evaluate on the Test Set
-------------------------
"""

def get_singular_values(trained_irmae):
    cov_latent = torch.zeros((latent_dims,latent_dims)).to(device)
    trained_irmae.eval()
    test_loss_avg, num_batches = 0, 0
    for image_batch, _ in test_dataloader:
        
        with torch.no_grad():

            image_batch = image_batch.to(device)

            # autoencoder reconstruction
            latent = trained_irmae.encoder(image_batch)
            min_rank_latent = trained_irmae.lnn(latent)
            cov_latent += min_rank_latent.T @ min_rank_latent
            image_batch_recon = trained_irmae(image_batch)

            # reconstruction error
            loss = F.mse_loss(image_batch_recon, image_batch)

            test_loss_avg += loss.item()
            num_batches += 1
        
    test_loss_avg /= num_batches
    print('average reconstruction error: %f' % (test_loss_avg))
    _,d,_ = torch.svd(cov_latent)
    return cov_latent, (d/d.max()).cpu()

standard_cov, standard_ae_singular_values = get_singular_values(standard_ae)
linear_cov, linear_singular_values = get_singular_values(linear_irmae)

print(torch.matrix_rank(standard_cov))
print(torch.matrix_rank(linear_cov))

"""Surprisingly, the rank of the latent space covariance matrix for the network with linear layers inserted is much less than of the standard variant without any linear layer inserted. This is contradictory to the results presented by many papers that show that SGD causes the implicit rank minimization effect. Yet, here we see that network with weights which are completely frozen from the beginning and never updated using SGD still show rank minimization property."""

from matplotlib.pyplot import figure
figure(num=None, figsize=(9, 7), dpi=80, facecolor='w', edgecolor='k')
plt.plot(standard_ae_singular_values, label=f'Standard AE, Latent space dimension - {torch.matrix_rank(standard_cov)}')
plt.plot(linear_singular_values, label=f'Linear, Latent space dimension - {torch.matrix_rank(linear_cov)}')
plt.ylabel('Singular Values')
plt.xlabel('Singular Value Rank')
plt.legend(loc="upper right")
plt.ylim(0,0.2)

"""Visualize Reconstructions
--------------------------
"""

# autoencoder = standard_ae
autoencoder = linear_irmae

import numpy as np
import matplotlib.pyplot as plt
plt.ion()

import torchvision.utils

autoencoder.eval()

# This function takes as an input the images to reconstruct
# and the name of the model with which the reconstructions
# are performed
def to_img(x):
    x = 0.5 * (x + 1)
    x = x.clamp(0, 1)
    return x

def show_image(img):
    img = to_img(img)
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))

def visualise_output(images, model):

    with torch.no_grad():

        images = images.to(device)
        images = model(images)
        images = images.cpu()
        images = to_img(images)
        np_imagegrid = torchvision.utils.make_grid(images[1:50], 10, 5).numpy()
        plt.imshow(np.transpose(np_imagegrid, (1, 2, 0)))
        plt.show()

images, labels = iter(test_dataloader).next()

# First visualise the original images
print('Original images')
show_image(torchvision.utils.make_grid(images[1:50],10,5))
plt.show()

# Reconstruct and visualise the images using the autoencoder
print('Autoencoder reconstruction:')
visualise_output(images, autoencoder)

"""Interpolate in Latent Space
----------------------------
"""

autoencoder.eval()

def interpolation(lambda1, model, img1, img2):
    
    with torch.no_grad():

        # latent vector of first image
        img1 = img1.to(device)
        latent_1 = model.encoder(img1)
        latent_1 = model.lnn(latent_1)
        

        # latent vector of second image
        img2 = img2.to(device)
        latent_2 = model.encoder(img2)
        latent_2 = model.lnn(latent_2)

        # interpolation of the two latent vectors
        inter_latent = lambda1 * latent_1 + (1- lambda1) * latent_2

        # reconstruct interpolated image
        inter_image = model.decoder(inter_latent)
        inter_image = inter_image.cpu()
    
    return inter_image
    
# sort part of test set by digit
digits = [[] for _ in range(10)]
for img_batch, label_batch in test_dataloader:
    for i in range(img_batch.size(0)):
        digits[label_batch[i]].append(img_batch[i:i+1])
    if sum(len(d) for d in digits) >= 1000:
        break;

# interpolation lambdas
lambda_range=np.linspace(0,1,10)

fig, axs = plt.subplots(2,5, figsize=(15, 6))
fig.subplots_adjust(hspace = .5, wspace=.001)
axs = axs.ravel()


for ind,l in enumerate(lambda_range):
    inter_image=interpolation(float(l), autoencoder, digits[5][0], digits[4][45])
  
    inter_image = to_img(inter_image)
    
    image = inter_image.numpy()
  
    axs[ind].imshow(image[0,0,:,:], cmap='gray')
    axs[ind].set_title('lambda_val='+str(round(l,1)))

plt.show()

